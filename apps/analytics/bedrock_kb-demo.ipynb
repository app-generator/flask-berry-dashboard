{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to query \n",
    "\n",
    "1) Amazon Bedrock LLM / Foundation Model and then \n",
    "\n",
    "2) enhance with Retreival Augmented Generation (RAG) from pre-prepared knowledge base from LDFE job status reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://youtu.be/BXgaK8PPZAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude (Anthropic Model) - after use case approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: using Anthropic Claude 2.1 for Amazon Bedrock to propose a list of questions to ask in relation to data anomalies for forecasting time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name = 'us-east-1')\n",
    "\n",
    "# Start with the query\n",
    "# *check* possibly mention labour demand forecasting \n",
    "prompt = \"For forecasting time series data, what are the key questions to ask in relation to data anomalies ?\"\n",
    "\n",
    "claude_llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    model_kwargs={\"temperature\": 0, \"top_k\": 10, \"max_tokens_to_sample\": 3000},\n",
    "    client=bedrock_client,\n",
    ")\n",
    "\n",
    "# Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = claude_llm(prompt)\n",
    "questions = [\n",
    "    item.split(\".\")[1].strip() for item in response.strip().split(\"\\n\\n\")[1:-1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Step 2: (fully managed RAG) - use native Knowledge Bases (based on s3 bucket with LDFE training and inference job statuses) for Amazon Bedrock RetrieveAndGenerate API to obtain answers directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id = \"O3SYXISL4R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\", region_name = 'us-east-1')\n",
    "\n",
    "def retrieveAndGenerate(\n",
    "    input: str,\n",
    "    kbId: str,\n",
    "    region: str = \"us-east-1\",\n",
    "    sessionId: str = None,\n",
    "    model_id: str = \"anthropic.claude-v2\", # changed from \"anthropic.claude-v2:1\",\n",
    "):\n",
    "    model_arn = f\"arn:aws:bedrock:{region}::foundation-model/{model_id}\"\n",
    "\n",
    "    if sessionId:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\"text\": input},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    \"knowledgeBaseId\": kbId,\n",
    "                    \"modelArn\": model_arn,\n",
    "                },\n",
    "            },\n",
    "            sessionId=sessionId,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\"text\": input},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    \"knowledgeBaseId\": kbId,\n",
    "                    \"modelArn\": model_arn,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "#target_qu = \"What are the key anomalies we are seeing in the model training process ?\"\n",
    "target_qu = \"What are the main errors we are seeing in the inference process ?\"\n",
    "\n",
    "\n",
    "response = retrieveAndGenerate(\n",
    "    target_qu, kb_id\n",
    ")\n",
    "\n",
    "generated_text = response[\"output\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main errors seen in the inference process are insufficient rota data errors and no model available errors.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get custom responses to the question posed in the last cell\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*THE ABOVE RESPONSE SOUNDS GOOD AND IS ACTUALLY CORRECT ON CHECKING THE SOURCE DATA AND LOOKING AT THE \"NOTE\" FIELD *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cite evidence for the above response\n",
    "response[\"citations\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = response.get('sessionId', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow up question with no context\n",
    "\n",
    "#followUpQu = \"how many jobs failed with the train_size None error\" # training\n",
    "followUpQu = \"is the same error the main cause of runs failing this morning ?\" # inference\n",
    "\n",
    "# more detailed\n",
    "#followUpQu = \"can you tell me approximately how many training jobs failed within the last month with the train_size=None error and what percentage this is of all the training jobs in that period\"\n",
    "\n",
    "retrieveAndGenerate(followUpQu, kb_id)[\"output\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THE ABOVE RESPONSE IS POOR ON A FEW LEVELS - IT SHOULD START \"NN....\" as it goes on to highlight a different error than the original one. Also on checking the source of the KB, the answer is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: 1 validation error detected: Value 'arn:aws:bedrock:6624af3f-f0f8-4936-b762-9ea730de90d8::foundation-model/anthropic.claude-v2' at 'retrieveAndGenerateConfiguration.knowledgeBaseConfiguration.modelArn' failed to satisfy constraint: Member must satisfy regular expression pattern: (arn:aws(-[^:]+)?:bedrock:[a-z0-9-]{1,20}:(([0-9]{12}:custom-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}/[a-z0-9]{12})|(:foundation-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|([0-9]{12}:provisioned-model/[a-z0-9]{12})))|([a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|(([0-9a-zA-Z][_-]?)+)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get context-specific reponse (i.e. considering original response)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# *check* throws an error atm about use of wildcards in sess_id ?\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sess_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mretrieveAndGenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfollowUpQu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkb_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msess_id\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mretrieveAndGenerate\u001b[0;34m(input, kbId, region, sessionId, model_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bedrock_agent_client\u001b[38;5;241m.\u001b[39mretrieve_and_generate(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m},\n\u001b[1;32m     15\u001b[0m         retrieveAndGenerateConfiguration\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         sessionId\u001b[38;5;241m=\u001b[39msessionId,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbedrock_agent_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_and_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrieveAndGenerateConfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKNOWLEDGE_BASE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkbId\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelArn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_arn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rotaready/rr_repos/ML-LabourDemandForecasting/.venv/lib/python3.12/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rotaready/rr_repos/ML-LabourDemandForecasting/.venv/lib/python3.12/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: 1 validation error detected: Value 'arn:aws:bedrock:6624af3f-f0f8-4936-b762-9ea730de90d8::foundation-model/anthropic.claude-v2' at 'retrieveAndGenerateConfiguration.knowledgeBaseConfiguration.modelArn' failed to satisfy constraint: Member must satisfy regular expression pattern: (arn:aws(-[^:]+)?:bedrock:[a-z0-9-]{1,20}:(([0-9]{12}:custom-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}/[a-z0-9]{12})|(:foundation-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|([0-9]{12}:provisioned-model/[a-z0-9]{12})))|([a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|(([0-9a-zA-Z][_-]?)+)"
     ]
    }
   ],
   "source": [
    "# get context-specific reponse (i.e. considering original response)\n",
    "# *check* throws an error atm about use of wildcards in sess_id ?\n",
    "sess_id = response[\"sessionId\"]\n",
    "retrieveAndGenerate(followUpQu, kb_id, sess_id)[\"output\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: RAG customisation - uses the Retrieve API to fetch the relevant chunks based on your query and pass it to any LLM provided by Amazon Bedrock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, kbId: str, numberOfResults: int = 5):\n",
    "    return bedrock_agent_client.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\"numberOfResults\": numberOfResults}\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 get the correponding KB context for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many failed inference jobs today mentioned busyness ?\"\n",
    "response = retrieve(query, kb_id, 3)\n",
    "retrievalResults = response[\"retrievalResults\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 extract the context for the LLM / FM prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults:\n",
    "        contexts.append(retrievedResult[\"content\"][\"text\"])\n",
    "    return \" \".join(contexts)\n",
    "\n",
    "contexts = get_contexts(retrievalResults)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 set up the LLM / FM in-context question-answering prompt template, then generate the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# # question NOT query_str for langchain step4 - see https://stackoverflow.com/questions/77839844/langchain-retrievalqa-missing-some-input-keys\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI system working on forecast anomalies, and provides answers to questions \\\n",
    "by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "claude_prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"] # question NOT query_str for langchain step4 - see https://stackoverflow.com/questions/77839844/langchain-retrievalqa-missing-some-input-keys\n",
    ")\n",
    "\n",
    "prompt = claude_prompt.format(context=contexts, question=query) # question NOT query_str for langchain step4 - see https://stackoverflow.com/questions/77839844/langchain-retrievalqa-missing-some-input-keys\n",
    "response = claude_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Amazon Bedrock LangChain integration - basically step 3 *check* integrated with the LangChain framework designed to simplify the creation of applications using large language models. So used to create an end-to-end customized Q&A application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 set up the LangChain retriever and specificy number of results to return\n",
    "\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id,\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 set up the LangChain RetrievalQA and generate answers from the knowledge base:\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=claude_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": claude_prompt}, # same prompt template as in step 3\n",
    ")\n",
    "\n",
    "#[qa(q)[\"result\"] for q in questions]\n",
    "\n",
    "[qa(query)[\"result\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Titan Express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/getting-started-with-amazon-titan-text-embeddings/\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/titan-models.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB text EMBEDDINGS refer to the NLP technique that converts textual data into numerical vectors that can be processed by ML algos and LLMs.\n",
    "# They are more suited for use cases where SEMANTICS are important (capturing meaning of singular words or phrases)\n",
    "\n",
    "# *check* differences between Amazon Titan text and embeddings models. Embeddings seems to be cheaper, possibly due to the storage of the numercal/vector representation of text, as opposed to the untransformed text format, but link below suggests (vanilla) text model is high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out avaialble Amazon Titan LLM models\n",
    "\n",
    "import boto3\n",
    "import json\n",
    " \n",
    "#Create the connection to Bedrock\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "# Let's see all available Amazon Models\n",
    "available_models = bedrock.list_foundation_models()\n",
    " \n",
    "for model in available_models['modelSummaries']:\n",
    "  if 'amazon' in model['modelId']:\n",
    "    print(model)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering example - have Amazon Titan write a poem about apples\n",
    "\n",
    "# Define prompt and model parameters\n",
    "prompt_data = \"\"\"Write me a poem about apples\"\"\"\n",
    " \n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,\n",
    "})\n",
    " \n",
    "model_id = 'amazon.titan-text-express-v1' #'amazon.titan-embed-text-v1' #look for embeddings in the modelID\n",
    "accept = 'application/json' \n",
    "content_type = 'application/json'\n",
    " \n",
    "# Invoke model \n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, \n",
    "    modelId=model_id, \n",
    "    accept=accept, \n",
    "    contentType=content_type\n",
    ")\n",
    " \n",
    "# Print response\n",
    "response_body = json.loads(response['body'].read())\n",
    "\n",
    "print(response_body)\n",
    "\n",
    "# embedding = response_body.get('embedding')\n",
    " \n",
    "# #Print the Embedding\n",
    " \n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response_body.get('results')\n",
    "results[0]['outputText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code sample taken from:\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Shows how to create a list of action items from a meeting transcript\n",
    "with the Amazon &titan-text-express; model (on demand).\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "class ImageError(Exception):\n",
    "    \"Custom exception for errors returned by Amazon &titan-text-express; model\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using Amazon &titan-text-express; model on demand.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        response (json): The response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        \"Generating text with Amazon &titan-text-express; model %s\", model_id)\n",
    "\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    finish_reason = response_body.get(\"error\")\n",
    "\n",
    "    if finish_reason is not None:\n",
    "        raise ImageError(f\"Text generation error. Error is {finish_reason}\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Amazon &titan-text-express; model %s\", model_id)\n",
    "\n",
    "    return response_body\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Amazon &titan-text-express; example.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.basicConfig(level=logging.INFO,\n",
    "                            format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "        model_id = 'amazon.titan-text-express-v1'\n",
    "\n",
    "        prompt = \"\"\"Meeting transcript: Miguel: Hi Brant, I want to discuss the workstream  \n",
    "            for our new product launch Brant: Sure Miguel, is there anything in particular you want\n",
    "            to discuss? Miguel: Yes, I want to talk about how users enter into the product.\n",
    "            Brant: Ok, in that case let me add in Namita. Namita: Hey everyone \n",
    "            Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "            Miguel: its too complicated and we should remove friction.  \n",
    "            for example, why do I need to fill out additional forms?  \n",
    "            I also find it difficult to find where to access the product\n",
    "            when I first land on the landing page. Brant: I would also add that\n",
    "            I think there are too many steps. Namita: Ok, I can work on the\n",
    "            landing page to make the product more discoverable but brant\n",
    "            can you work on the additonal forms? Brant: Yes but I would need \n",
    "            to work with James from another team as he needs to unblock the sign up workflow.\n",
    "            Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "            Miguel: Sure.\n",
    "            From the meeting transcript above, Create a list of action items for each person. \"\"\"\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"inputText\": prompt,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": 4096,\n",
    "                \"stopSequences\": [],\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 1\n",
    "            }\n",
    "        })\n",
    "\n",
    "        response_body = generate_text(model_id, body)\n",
    "        print(f\"Input token count: {response_body['inputTextTokenCount']}\")\n",
    "\n",
    "        for result in response_body['results']:\n",
    "            print(f\"Token count: {result['tokenCount']}\")\n",
    "            print(f\"Output text: {result['outputText']}\")\n",
    "            print(f\"Completion reason: {result['completionReason']}\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "              format(message))\n",
    "    except ImageError as err:\n",
    "        logger.error(err.message)\n",
    "        print(err.message)\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished generating text with the Amazon &titan-text-express; model {model_id}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "#bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "bedrock_client = boto3.client(service_name='bedrock',\n",
    "                              region_name='us-east-1',\n",
    "                              endpoint_url='https://bedrock.us-east-1.amazonaws.com',)\n",
    "\n",
    "# Start with the query\n",
    "# *check* possibly mention labour demand forecasting \n",
    "prompt = \"For forecasting time series data, what are the key questions to ask in relation to data anomalies ?\"\n",
    "\n",
    "titan_llm = Bedrock(\n",
    "    model_id=\"amazon.titan-text-express-v1\", # modelId on AWS Console > Bedrock > Providers > https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=amazon.titan-text-express-v1\n",
    "    model_kwargs={\"temperature\": 0, \"top_k\": 10, \"max_tokens_to_sample\": 3000},\n",
    "    client=bedrock_client,\n",
    ")\n",
    "\n",
    "# Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = titan_llm(prompt)\n",
    "questions = [\n",
    "    item.split(\".\")[1].strip() for item in response.strip().split(\"\\n\\n\")[1:-1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foundation_model(model_identifier):\n",
    "    \"\"\"\n",
    "    Get details about an Amazon Bedrock foundation model.\n",
    "\n",
    "    :return: The foundation model's details.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return bedrock_client.get_foundation_model(\n",
    "            modelIdentifier=model_identifier\n",
    "        )[\"modelDetails\"]\n",
    "    except ClientError:\n",
    "        logger.error(\n",
    "            f\"Couldn't get foundation models details for {model_identifier}\"\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_foundation_model('titan_llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    credentials_profile_name=\"default\",\n",
    "    model_id=\"amazon.titan-text-express-v1\"\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "result = conversation.predict(input=\"What is the most exciting question you can ask an LLM?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://aws.amazon.com/blogs/machine-learning/getting-started-with-amazon-titan-text-embeddings/\n",
    "\n",
    "import boto3\n",
    "import json\n",
    " \n",
    "#Create the connection to Bedrock\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "# Let's see all available Amazon Models\n",
    "available_models = bedrock.list_foundation_models()\n",
    " \n",
    "for model in available_models['modelSummaries']:\n",
    "  if 'amazon' in model['modelId']:\n",
    "    print(model)\n",
    " \n",
    "# Define prompt and model parameters\n",
    "prompt_data = \"\"\"Write me a poem about apples\"\"\"\n",
    " \n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,\n",
    "})\n",
    " \n",
    "model_id = 'amazon.titan-text-express-v1' #'amazon.titan-embed-text-v1' #look for embeddings in the modelID\n",
    "accept = 'application/json' \n",
    "content_type = 'application/json'\n",
    " \n",
    "# Invoke model \n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, \n",
    "    modelId=model_id, \n",
    "    accept=accept, \n",
    "    contentType=content_type\n",
    ")\n",
    " \n",
    "# Print response\n",
    "response_body = json.loads(response['body'].read())\n",
    "embedding = response_body.get('embedding')\n",
    " \n",
    "#Print the Embedding\n",
    " \n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
