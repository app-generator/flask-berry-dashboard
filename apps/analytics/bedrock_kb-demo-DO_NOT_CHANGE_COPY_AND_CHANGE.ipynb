{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to query \n",
    "\n",
    "1) Amazon Bedrock LLM / Foundation Model and then \n",
    "\n",
    "2) enhance with Retreival Augmented Generation (RAG) from pre-prepared knowledge base from LDFE job status reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://youtu.be/BXgaK8PPZAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude (Anthropic Model) - after use case approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: using Anthropic Claude 2.1 for Amazon Bedrock to propose a list of questions to ask in relation to data anomalies for forecasting time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barry.walsh/rotaready/rr_repos/ML-LabourDemandForecasting/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name = 'us-east-1')\n",
    "\n",
    "# Start with the query\n",
    "# *check* possibly mention labour demand forecasting \n",
    "prompt = \"For forecasting time series data, what are the key questions to ask in relation to data anomalies ?\"\n",
    "\n",
    "claude_llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    model_kwargs={\"temperature\": 0, \"top_k\": 10, \"max_tokens_to_sample\": 3000},\n",
    "    client=bedrock_client,\n",
    ")\n",
    "\n",
    "# Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = claude_llm(prompt)\n",
    "questions = [\n",
    "    item.split(\".\")[1].strip() for item in response.strip().split(\"\\n\\n\")[1:-1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's important to identify and potentially remove or adjust any anomalous data points\",\n",
       " 'Consider whether and how to fill in or otherwise adjust for any missing values',\n",
       " '',\n",
       " 'Plotting the data over time can help spot changes in seasonality',\n",
       " 'This may require using autoregressive forecasting models',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Step 2: (fully managed RAG) - use native Knowledge Bases (based on s3 bucket with LDFE training and inference job statuses) for Amazon Bedrock RetrieveAndGenerate API to obtain answers directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_id = \"us-east-1\" # for now only this supported\n",
    "kb_id = \"O3SYXISL4R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\", region_name = reg_id)\n",
    "\n",
    "def retrieveAndGenerate(\n",
    "    input: str,\n",
    "    kbId: str,\n",
    "    region: str = reg_id,\n",
    "    sessionId: str = None,\n",
    "    model_id: str = \"anthropic.claude-v2\", # changed from \"anthropic.claude-v2:1\",\n",
    "):\n",
    "    model_arn = f\"arn:aws:bedrock:{region}::foundation-model/{model_id}\"\n",
    "\n",
    "    if sessionId:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\"text\": input},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    \"knowledgeBaseId\": kbId,\n",
    "                    \"modelArn\": model_arn,\n",
    "                },\n",
    "            },\n",
    "            sessionId=sessionId,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\"text\": input},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    \"knowledgeBaseId\": kbId,\n",
    "                    \"modelArn\": model_arn,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "#target_qu = \"What are the key anomalies we are seeing in the model training process ?\"\n",
    "target_qu = \"What are the main errors we are seeing in the inference process ?\"\n",
    "\n",
    "\n",
    "response = retrieveAndGenerate(\n",
    "    target_qu, kb_id\n",
    ")\n",
    "\n",
    "generated_text = response[\"output\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main errors we are seeing in the inference process are insufficient rota data and no model or proxy model available in S3.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get custom responses to the question posed in the last cell\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*THE ABOVE RESPONSE SOUNDS GOOD AND IS ACTUALLY CORRECT ON CHECKING THE SOURCE DATA AND LOOKING AT THE \"NOTE\" FIELD *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generatedResponsePart': {'textResponsePart': {'span': {'end': 125,\n",
       "     'start': 0},\n",
       "    'text': 'The main errors we are seeing in the inference process are insufficient rota data and no model or proxy model available in S3.'}},\n",
       "  'retrievedReferences': [{'content': {'text': '31424,13/03/2024 03:08,brewdog,haw,inference,success,\"[\\'\\', \\'\\']\", 31425,13/03/2024 03:08,brewdog,brs,inference,success,\"[\\'\\', \\'\\']\", 31426,13/03/2024 03:08,brewdog,heo,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"brewdog-heo has insufficient data, no model\" 31427,13/03/2024 03:08,brewdog,wac,inference,success,\"[\\'\\', \\'\\']\", 31428,13/03/2024 03:09,brewdog,can,inference,success,\"[\\'\\', \\'\\']\", 31429,13/03/2024 03:09,brewdog,mii,inference,success,\"[\\'\\', \\'\\']\", 31430,13/03/2024 03:09,brewdog,car,inference,success,\"[\\'\\', \\'\\']\", 31431,13/03/2024 03:09,brewdog,nic,inference,success,\"[\\'\\', \\'\\']\", 31432,13/03/2024 03:09,brewdog,mil,inference,success,\"[\\'\\', \\'\\']\", 31433,13/03/2024'},\n",
       "    'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "     'type': 'S3'},\n",
       "    'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'}},\n",
       "   {'content': {'text': ',\"[\\'\\', \\'\\']\", 25265,12/02/2024 11:05,thehotelfolk,swh,inference,success,\"[\\'\\', \\'\\']\", 25266,12/02/2024 11:05,warnerleisure,tht,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"insufficient data, no model\" 25267,12/02/2024 11:05,nq64,soh,inference,success,\"[\\'\\', \\'\\']\", 25268,12/02/2024 11:05,warnerleisure,tho,inference,success,\"[\\'\\', \\'\\']\", 25269,12/02/2024 11:05,brewdog,exe,inference,success,\"[\\'\\', \\'\\']\", 25270,12/02/2024 11:05,thehotelfolk,swk,inference,success,\"[\\'\\', \\'\\']\", 25271,12/02/2024 11:05,namco,nob,inference,success,\"[\\'\\', \\'\\']\", 25272,12/02/2024 11:05,pizzapilgrims,bri,inference,success,\"[\\'\\', \\'\\']\", 25273,12/02/2024 11:05,brewdog,dbc,inference,success,\"[\\'\\', \\'\\']\",'},\n",
       "    'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "     'type': 'S3'},\n",
       "    'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'}},\n",
       "   {'content': {'text': '\\', \\'\\']\", 31925,16/03/2024 03:11,brewdog,per,inference,success,\"[\\'\\', \\'\\']\", 31926,16/03/2024 03:11,brewdog,rop,inference,success,\"[\\'\\', \\'\\']\", 31927,16/03/2024 03:11,brewdog,sap,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"brewdog-sap has insufficient data, no model\" 31928,16/03/2024 03:11,maray,bsf,inference,success,\"[\\'\\', \\'\\']\", 31929,16/03/2024 03:11,maray,heo,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"maray-heo has insufficient data, no model\" 31930,16/03/2024 03:11,brewdog,wak,inference,success,\"[\\'\\', \\'\\']\", 31931,16/03/2024 03:11,maray,mab,inference,success,\"[\\'\\', \\'\\']\", 31932,16/03/2024 03:11,brewdog,wac,inference,success,\"[\\'\\', \\'\\']\", 31933,16/03/2024'},\n",
       "    'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "     'type': 'S3'},\n",
       "    'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'}},\n",
       "   {'content': {'text': '31152,12/03/2024 07:42,brownsofbrockley,std,inference,success,\"[\\'\\', \\'\\']\", 31153,12/03/2024 07:42,housecafes,raf,inference,success,\"[\\'\\', \\'\\']\", 31154,12/03/2024 07:42,warnerleisure,scm,inference,success,\"[\\'\\', \\'\\']\", 31155,12/03/2024 07:42,thehotelfolk,fin,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"thehotelfolk-fin has insufficient data, no model\" 31156,12/03/2024 07:42,brewdog,not,inference,success,\"[\\'\\', \\'\\']\", 31157,12/03/2024 07:42,itison,epi,inference,success,\"[\\'\\', \\'\\']\", 31158,12/03/2024 07:42,brewdog,nrk,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"brewdog-nrk has insufficient data, no model\" 31159,12/03/2024 07:42,thehotelfolk,wlr,inference,success,\"[\\'\\', \\'\\']\", 31160,12/03/2024 07:42,warnerleisure,cro,inference,success,\"[\\'\\''},\n",
       "    'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "     'type': 'S3'},\n",
       "    'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'}},\n",
       "   {'content': {'text': '\\'\\', \\'\\']\", 31797,16/03/2024 03:05,gusto,man,inference,success,\"[\\'\\', \\'\\']\", 31798,16/03/2024 03:05,signaturepubs,aul,inference,success,\"[\\'\\', \\'\\']\", 31799,16/03/2024 03:06,gusto,edi,inference,success,\"[\\'\\', \\'\\']\", 31800,16/03/2024 03:06,signaturepubs,chu,inference,success,\"[\\'\\', \\'\\']\", 31801,16/03/2024 03:06,signaturepubs,spi,inference,success,\"[\\'\\', \\'\\']\", 31802,16/03/2024 03:06,gusto,heo,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"gusto-heo has insufficient data, no model\" 31803,16/03/2024 03:06,signaturepubs,cop,inference,success,\"[\\'\\', \\'\\']\", 31804,16/03/2024 03:06,maray,dsf,inference,success,\"[\\'\\', \\'\\']\", 31805,16/03/2024 03:06,gusto,hes,inference,success,\"[\\'\\', \\'\\']\",'},\n",
       "    'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "     'type': 'S3'},\n",
       "    'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'}}]}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cite evidence for the above response\n",
    "response[\"citations\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = response.get('sessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23bd3704-8be1-4cab-ad91-aaaeca7d9ad2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow up question with no context\n",
    "\n",
    "#followUpQu = \"how many jobs failed with the train_size None error\" # training\n",
    "followUpQu = \"is the same error the main cause of runs failing this morning ?\" # inference\n",
    "\n",
    "# more detailed\n",
    "#followUpQu = \"can you tell me approximately how many training jobs failed within the last month with the train_size=None error and what percentage this is of all the training jobs in that period\"\n",
    "\n",
    "retrieveAndGenerate(followUpQu, kb_id)[\"output\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THE ABOVE RESPONSE IS POOR ON A FEW LEVELS - IT SHOULD START \"No....\" as it goes on to highlight a different error than the original one. Also on checking the source of the KB, the answer is incorrect. It also assumes the user is still referring to inference runs, rather than both training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, insufficient rota data seems to be the main cause of inference run failures this morning.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get context-specific reponse (i.e. considering original response)\n",
    "# *check* throws an error atm about use of wildcards in sess_id ?\n",
    "sess_id = response[\"sessionId\"]\n",
    "retrieveAndGenerate(followUpQu, kb_id, reg_id, sess_id)[\"output\"][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THIS RESPONSE RETAINS CONTEXT AND WHILE A BIT SPARSE, FOLLOWS ON FROM THE ORIGINAL QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Step 3: RAG customisation - uses the Retrieve API to fetch the relevant chunks based on your query and pass it to any LLM provided by Amazon Bedrock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, kbId: str, numberOfResults: int = 5):\n",
    "    return bedrock_agent_client.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\"numberOfResults\": numberOfResults}\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 get the correponding KB context for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many failed inference jobs today mentioned busyness ?\"\n",
    "response = retrieve(query, kb_id, 3)\n",
    "retrievalResults = response[\"retrievalResults\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': {'text': ', \\'\\', \\'\\']\", 32433,19/03/2024 09:02,nq64,car,inference,success,\"[\\'\\', \\'inconsistent busyness data - check\\', \\'\\']\", 32434,19/03/2024 09:02,brewdog,che,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32435,19/03/2024 09:02,warnerleisure,ahs,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 32436,19/03/2024 09:02,spaceandthyme,kit,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 32437,19/03/2024 09:02,brewdog,cbg,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32438,19/03/2024 09:02,brewdog,bou,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32439,19/03/2024 09:02,namco,nwc,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32440,19/03/2024 09:02,bonnieandwild,kps,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32441,19/03/2024'},\n",
       "  'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "   'type': 'S3'},\n",
       "  'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "  'score': 0.5469439},\n",
       " {'content': {'text': '[\\'\\', \\'inconsistent busyness data - check\\', \\'\\']\", 42421,20/04/2024 03:05,warnerleisure,cog,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"warnerleisure-cog has insufficient data, no model\" 42422,20/04/2024 03:05,brewdog,bwi,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 42423,20/04/2024 03:05,thehotelfolk,wlk,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 42424,20/04/2024 03:05,brewdog,cas,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 42425,20/04/2024 03:05,brewdog,dhb,inference,success,\"[\\'\\', \\'inconsistent busyness data - check\\', \\'\\']\", 42426,20/04/2024 03:05,pizzapilgrims,eus,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"pizzapilgrims-eus has insufficient data, no model\" 42427,20/04/2024 03:05,brewdog,bir,inference,success,\"[\\'\\', \\'\\', \\'\\']\",'},\n",
       "  'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "   'type': 'S3'},\n",
       "  'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "  'score': 0.5417805},\n",
       " {'content': {'text': ', \\'\\']\", 39126,09/04/2024 03:07,thehotelfolk,brc,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39127,09/04/2024 03:07,brewdog,per,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 39128,09/04/2024 03:07,roseacre,bea,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39129,09/04/2024 03:07,warnerleisure,adr,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39130,09/04/2024 03:08,brewdog,man,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39131,09/04/2024 03:08,warnerleisure,thh,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39132,09/04/2024 03:08,brewdog,mii,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 39133,09/04/2024 03:08,roseacre,bee,inference,fail,\"[\\'\\', \\'inconsistent busyness data -'},\n",
       "  'location': {'s3Location': {'uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "   'type': 'S3'},\n",
       "  'metadata': {'x-amz-bedrock-kb-source-uri': 's3://ldfe-genai/debug/inference/job_status.csv'},\n",
       "  'score': 0.5412207}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrievalResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 extract the context for the LLM / FM prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', \\'\\', \\'\\']\", 32433,19/03/2024 09:02,nq64,car,inference,success,\"[\\'\\', \\'inconsistent busyness data - check\\', \\'\\']\", 32434,19/03/2024 09:02,brewdog,che,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32435,19/03/2024 09:02,warnerleisure,ahs,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 32436,19/03/2024 09:02,spaceandthyme,kit,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 32437,19/03/2024 09:02,brewdog,cbg,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32438,19/03/2024 09:02,brewdog,bou,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32439,19/03/2024 09:02,namco,nwc,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32440,19/03/2024 09:02,bonnieandwild,kps,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 32441,19/03/2024 [\\'\\', \\'inconsistent busyness data - check\\', \\'\\']\", 42421,20/04/2024 03:05,warnerleisure,cog,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"warnerleisure-cog has insufficient data, no model\" 42422,20/04/2024 03:05,brewdog,bwi,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 42423,20/04/2024 03:05,thehotelfolk,wlk,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 42424,20/04/2024 03:05,brewdog,cas,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 42425,20/04/2024 03:05,brewdog,dhb,inference,success,\"[\\'\\', \\'inconsistent busyness data - check\\', \\'\\']\", 42426,20/04/2024 03:05,pizzapilgrims,eus,inference,fail,[\\'insufficient rota data - no model or proxy model in S3 available\\'],\"pizzapilgrims-eus has insufficient data, no model\" 42427,20/04/2024 03:05,brewdog,bir,inference,success,\"[\\'\\', \\'\\', \\'\\']\", , \\'\\']\", 39126,09/04/2024 03:07,thehotelfolk,brc,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39127,09/04/2024 03:07,brewdog,per,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 39128,09/04/2024 03:07,roseacre,bea,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39129,09/04/2024 03:07,warnerleisure,adr,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39130,09/04/2024 03:08,brewdog,man,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39131,09/04/2024 03:08,warnerleisure,thh,inference,success,\"[\\'\\', \\'unable to capture google places data\\', \\'\\']\", 39132,09/04/2024 03:08,brewdog,mii,inference,success,\"[\\'\\', \\'\\', \\'\\']\", 39133,09/04/2024 03:08,roseacre,bee,inference,fail,\"[\\'\\', \\'inconsistent busyness data -'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults:\n",
    "        contexts.append(retrievedResult[\"content\"][\"text\"])\n",
    "    return \" \".join(contexts)\n",
    "\n",
    "contexts = get_contexts(retrievalResults)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 set up the LLM / FM in-context question-answering prompt template, then generate the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# # question NOT query_str for langchain step4 - see https://stackoverflow.com/questions/77839844/langchain-retrievalqa-missing-some-input-keys\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI system working on forecast anomalies, and provides answers to questions \\\n",
    "by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "claude_prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"] # question NOT query_str for langchain step4 - see https://stackoverflow.com/questions/77839844/langchain-retrievalqa-missing-some-input-keys\n",
    ")\n",
    "\n",
    "prompt = claude_prompt.format(context=contexts, question=query) # question NOT query_str for langchain step4 - see https://stackoverflow.com/questions/77839844/langchain-retrievalqa-missing-some-input-keys\n",
    "response = claude_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Unfortunately, I do not have enough information to definitively state how many failed inference jobs today mentioned busyness. The context provided includes some failed inference jobs with messages about inconsistent busyness data, but does not specify the date these jobs ran or the total number of failed inference jobs. Without additional details on the total failed jobs and dates, I cannot provide a statistic on how many mentioned busyness.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THIS APPROACH IS MORE SOPHISTICATED AS IT USES THE CONTEXT SPECIFIC KNOWLEDGE BASE THEN, CHUNKS UP THE REVELANT VECTORS RELATING TO THE QUESTION, THEN HANDS-OFF TO THE LLM (ANTHROPIC) TO PROVIDE A MORE INFORMED / DETAILED RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Amazon Bedrock LangChain integration - basically step 3 *check* integrated with the LangChain framework designed to simplify the creation of applications using large language models. So used to create an end-to-end customized Q&A application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 set up the LangChain retriever and specificy number of results to return\n",
    "\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id,\n",
    "    region_name=\"us-east-1\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barry.walsh/rotaready/rr_repos/ML-LabourDemandForecasting/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" Unfortunately, there is no information provided in the context about failed inference jobs mentioning busyness today. The context shows inference job results from various dates, but does not give a count of failed jobs mentioning busyness on any specific date. So I don't have enough information to provide a numerical answer about failed inference jobs mentioning busyness today.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2 set up the LangChain RetrievalQA and generate answers from the knowledge base:\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=claude_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": claude_prompt}, # same prompt template as in step 3\n",
    ")\n",
    "\n",
    "#[qa(q)[\"result\"] for q in questions]\n",
    "\n",
    "[qa(query)[\"result\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THIS APPROACH IS I THINK THE PREFERRED APPROACH AS USES LANGCHAIN SIMPLIFIED INTEGRATION FOR GENAI APPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Titan Express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/getting-started-with-amazon-titan-text-embeddings/\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/titan-models.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB text EMBEDDINGS refer to the NLP technique that converts textual data into numerical vectors that can be processed by ML algos and LLMs.\n",
    "# They are more suited for use cases where SEMANTICS are important (capturing meaning of singular words or phrases)\n",
    "\n",
    "# *check* differences between Amazon Titan text and embeddings models. Embeddings seems to be cheaper, possibly due to the storage of the numercal/vector representation of text, as opposed to the untransformed text format, but link below suggests (vanilla) text model is high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out avaialble Amazon Titan LLM models\n",
    "\n",
    "import boto3\n",
    "import json\n",
    " \n",
    "#Create the connection to Bedrock\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "# Let's see all available Amazon Models\n",
    "available_models = bedrock.list_foundation_models()\n",
    " \n",
    "for model in available_models['modelSummaries']:\n",
    "  if 'amazon' in model['modelId']:\n",
    "    print(model)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering example - have Amazon Titan write a poem about apples\n",
    "\n",
    "# Define prompt and model parameters\n",
    "prompt_data = \"\"\"Write me a poem about apples\"\"\"\n",
    " \n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,\n",
    "})\n",
    " \n",
    "model_id = 'amazon.titan-text-express-v1' #'amazon.titan-embed-text-v1' #look for embeddings in the modelID\n",
    "accept = 'application/json' \n",
    "content_type = 'application/json'\n",
    " \n",
    "# Invoke model \n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, \n",
    "    modelId=model_id, \n",
    "    accept=accept, \n",
    "    contentType=content_type\n",
    ")\n",
    " \n",
    "# Print response\n",
    "response_body = json.loads(response['body'].read())\n",
    "\n",
    "print(response_body)\n",
    "\n",
    "# embedding = response_body.get('embedding')\n",
    " \n",
    "# #Print the Embedding\n",
    " \n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response_body.get('results')\n",
    "results[0]['outputText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code sample taken from:\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Shows how to create a list of action items from a meeting transcript\n",
    "with the Amazon &titan-text-express; model (on demand).\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "class ImageError(Exception):\n",
    "    \"Custom exception for errors returned by Amazon &titan-text-express; model\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using Amazon &titan-text-express; model on demand.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        response (json): The response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        \"Generating text with Amazon &titan-text-express; model %s\", model_id)\n",
    "\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    finish_reason = response_body.get(\"error\")\n",
    "\n",
    "    if finish_reason is not None:\n",
    "        raise ImageError(f\"Text generation error. Error is {finish_reason}\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Amazon &titan-text-express; model %s\", model_id)\n",
    "\n",
    "    return response_body\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Amazon &titan-text-express; example.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.basicConfig(level=logging.INFO,\n",
    "                            format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "        model_id = 'amazon.titan-text-express-v1'\n",
    "\n",
    "        prompt = \"\"\"Meeting transcript: Miguel: Hi Brant, I want to discuss the workstream  \n",
    "            for our new product launch Brant: Sure Miguel, is there anything in particular you want\n",
    "            to discuss? Miguel: Yes, I want to talk about how users enter into the product.\n",
    "            Brant: Ok, in that case let me add in Namita. Namita: Hey everyone \n",
    "            Brant: Hi Namita, Miguel wants to discuss how users enter into the product.\n",
    "            Miguel: its too complicated and we should remove friction.  \n",
    "            for example, why do I need to fill out additional forms?  \n",
    "            I also find it difficult to find where to access the product\n",
    "            when I first land on the landing page. Brant: I would also add that\n",
    "            I think there are too many steps. Namita: Ok, I can work on the\n",
    "            landing page to make the product more discoverable but brant\n",
    "            can you work on the additonal forms? Brant: Yes but I would need \n",
    "            to work with James from another team as he needs to unblock the sign up workflow.\n",
    "            Miguel can you document any other concerns so that I can discuss with James only once?\n",
    "            Miguel: Sure.\n",
    "            From the meeting transcript above, Create a list of action items for each person. \"\"\"\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"inputText\": prompt,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": 4096,\n",
    "                \"stopSequences\": [],\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 1\n",
    "            }\n",
    "        })\n",
    "\n",
    "        response_body = generate_text(model_id, body)\n",
    "        print(f\"Input token count: {response_body['inputTextTokenCount']}\")\n",
    "\n",
    "        for result in response_body['results']:\n",
    "            print(f\"Token count: {result['tokenCount']}\")\n",
    "            print(f\"Output text: {result['outputText']}\")\n",
    "            print(f\"Completion reason: {result['completionReason']}\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "              format(message))\n",
    "    except ImageError as err:\n",
    "        logger.error(err.message)\n",
    "        print(err.message)\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished generating text with the Amazon &titan-text-express; model {model_id}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "#bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "bedrock_client = boto3.client(service_name='bedrock',\n",
    "                              region_name='us-east-1',\n",
    "                              endpoint_url='https://bedrock.us-east-1.amazonaws.com',)\n",
    "\n",
    "# Start with the query\n",
    "# *check* possibly mention labour demand forecasting \n",
    "prompt = \"For forecasting time series data, what are the key questions to ask in relation to data anomalies ?\"\n",
    "\n",
    "titan_llm = Bedrock(\n",
    "    model_id=\"amazon.titan-text-express-v1\", # modelId on AWS Console > Bedrock > Providers > https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=amazon.titan-text-express-v1\n",
    "    model_kwargs={\"temperature\": 0, \"top_k\": 10, \"max_tokens_to_sample\": 3000},\n",
    "    client=bedrock_client,\n",
    ")\n",
    "\n",
    "# Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = titan_llm(prompt)\n",
    "questions = [\n",
    "    item.split(\".\")[1].strip() for item in response.strip().split(\"\\n\\n\")[1:-1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foundation_model(model_identifier):\n",
    "    \"\"\"\n",
    "    Get details about an Amazon Bedrock foundation model.\n",
    "\n",
    "    :return: The foundation model's details.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return bedrock_client.get_foundation_model(\n",
    "            modelIdentifier=model_identifier\n",
    "        )[\"modelDetails\"]\n",
    "    except ClientError:\n",
    "        logger.error(\n",
    "            f\"Couldn't get foundation models details for {model_identifier}\"\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_foundation_model('titan_llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    credentials_profile_name=\"default\",\n",
    "    model_id=\"amazon.titan-text-express-v1\"\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "result = conversation.predict(input=\"What is the most exciting question you can ask an LLM?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://aws.amazon.com/blogs/machine-learning/getting-started-with-amazon-titan-text-embeddings/\n",
    "\n",
    "import boto3\n",
    "import json\n",
    " \n",
    "#Create the connection to Bedrock\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1', \n",
    "    \n",
    ")\n",
    " \n",
    "# Let's see all available Amazon Models\n",
    "available_models = bedrock.list_foundation_models()\n",
    " \n",
    "for model in available_models['modelSummaries']:\n",
    "  if 'amazon' in model['modelId']:\n",
    "    print(model)\n",
    " \n",
    "# Define prompt and model parameters\n",
    "prompt_data = \"\"\"Write me a poem about apples\"\"\"\n",
    " \n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,\n",
    "})\n",
    " \n",
    "model_id = 'amazon.titan-text-express-v1' #'amazon.titan-embed-text-v1' #look for embeddings in the modelID\n",
    "accept = 'application/json' \n",
    "content_type = 'application/json'\n",
    " \n",
    "# Invoke model \n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, \n",
    "    modelId=model_id, \n",
    "    accept=accept, \n",
    "    contentType=content_type\n",
    ")\n",
    " \n",
    "# Print response\n",
    "response_body = json.loads(response['body'].read())\n",
    "embedding = response_body.get('embedding')\n",
    " \n",
    "#Print the Embedding\n",
    " \n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
